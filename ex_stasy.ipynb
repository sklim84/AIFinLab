{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# STaSy: Score-based Tabular Data Synthesis"
   ],
   "metadata": {
    "id": "8XDiA2PvEiox"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_RKjKghM8PW2",
    "ExecuteTime": {
     "end_time": "2024-06-02T09:11:15.109950Z",
     "start_time": "2024-06-02T09:11:14.994776Z"
    }
   },
   "source": [
    "#@title Install Git repository% \n",
    "%cd baselines\n",
    "!git clone https://github.com/JayoungKim408/STaSy\n",
    "%cd STaSy"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bigdyl/economics/AIFinLab/baselines\n",
      "fatal: destination path 'STaSy' already exists and is not an empty directory.\r\n",
      "/home/bigdyl/economics/AIFinLab/baselines/STaSy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bigdyl/anaconda3/envs/aifinlab/lib/python3.9/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "#@title Install required libraries\n",
    "\n",
    "!pip install --upgrade pip\n",
    "!pip install --upgrade setuptools\n",
    "!pip install ml_collections\n"
   ],
   "metadata": {
    "id": "rn2aqETWAN7j",
    "ExecuteTime": {
     "end_time": "2024-06-02T09:11:24.032449Z",
     "start_time": "2024-06-02T09:11:21.173967Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\r\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\r\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\r\n",
      "\u001B[33mDEPRECATION: Python 2.7 reached the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 is no longer maintained. pip 21.0 will drop support for Python 2.7 in January 2021. More details about Python 2 support in pip can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support pip 21.0 will remove support for this functionality.\u001B[0m\r\n",
      "Defaulting to user installation because normal site-packages is not writeable\r\n",
      "Requirement already up-to-date: pip in /home/bigdyl/.local/lib/python2.7/site-packages (20.3.4)\r\n",
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\r\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\r\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\r\n",
      "\u001B[33mDEPRECATION: Python 2.7 reached the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 is no longer maintained. pip 21.0 will drop support for Python 2.7 in January 2021. More details about Python 2 support in pip can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support pip 21.0 will remove support for this functionality.\u001B[0m\r\n",
      "Defaulting to user installation because normal site-packages is not writeable\r\n",
      "Requirement already up-to-date: setuptools in /home/bigdyl/.local/lib/python2.7/site-packages (44.1.1)\r\n",
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\r\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\r\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\r\n",
      "\u001B[33mDEPRECATION: Python 2.7 reached the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 is no longer maintained. pip 21.0 will drop support for Python 2.7 in January 2021. More details about Python 2 support in pip can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support pip 21.0 will remove support for this functionality.\u001B[0m\r\n",
      "Defaulting to user installation because normal site-packages is not writeable\r\n",
      "Collecting ml_collections\r\n",
      "  Using cached ml_collections-0.1.1.tar.gz (77 kB)\r\n",
      "\u001B[31m    ERROR: Command errored out with exit status 1:\r\n",
      "     command: /usr/bin/python -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-FMUhfH/ml-collections/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-FMUhfH/ml-collections/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base /tmp/pip-pip-egg-info-BrDSTP\r\n",
      "         cwd: /tmp/pip-install-FMUhfH/ml-collections/\r\n",
      "    Complete output (5 lines):\r\n",
      "    Traceback (most recent call last):\r\n",
      "      File \"<string>\", line 1, in <module>\r\n",
      "      File \"/tmp/pip-install-FMUhfH/ml-collections/setup.py\", line 18, in <module>\r\n",
      "        from setuptools import find_namespace_packages\r\n",
      "    ImportError: cannot import name find_namespace_packages\r\n",
      "    ----------------------------------------\u001B[0m\r\n",
      "\u001B[31mERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "#@title Import packages for score-based generative model\n",
    "\n",
    "import numpy as np\n",
    "# import tensorflow as tf\n",
    "import pandas as pd\n",
    "from models import ncsnpp_tabular\n",
    "import losses\n",
    "import likelihood\n",
    "import sampling as sampling_\n",
    "from models import utils as mutils\n",
    "from models.ema import ExponentialMovingAverage\n",
    "import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import evaluation\n",
    "import sde_lib\n",
    "from absl import flags\n",
    "import torch\n",
    "from utils import save_checkpoint, restore_checkpoint, apply_activate\n",
    "import collections\n",
    "from torch.utils import tensorboard\n",
    "import os\n",
    "from ml_collections import config_flags, config_dict"
   ],
   "metadata": {
    "id": "corqRLaS8WOw",
    "ExecuteTime": {
     "end_time": "2024-06-02T09:11:29.455660Z",
     "start_time": "2024-06-02T09:11:26.835184Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-02 09:11:27.934670: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "#@title Load configuration\n",
    "config = config_dict.ConfigDict()\n",
    "config.workdir = \"stasy\"\n",
    "config.seed = 42\n",
    "config.device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "config.training = training = config_dict.ConfigDict()\n",
    "training.batch_size = 900 # 1000\n",
    "training.epoch = 1 # 20\n",
    "training.likelihood_weighting = False\n",
    "training.continuous = True\n",
    "training.reduce_mean = False\n",
    "training.eps = 1e-05\n",
    "training.loss_weighting = False\n",
    "training.spl = True\n",
    "training.lambda_ = 0.5\n",
    "training.sde = 'vesde'\n",
    "training.n_iters = 100000\n",
    "training.tolerance = 1e-01  # 1e-3\n",
    "training.hutchinson_type = \"Rademacher\"\n",
    "training.retrain_type = \"median\"\n",
    "training.eps_iters = 1\n",
    "training.fine_tune_epochs = 1\n",
    "\n",
    "config.sampling = sampling = config_dict.ConfigDict()\n",
    "sampling.n_steps_each = 1\n",
    "sampling.noise_removal = False\n",
    "sampling.probability_flow = True\n",
    "sampling.snr = 0.16\n",
    "sampling.method = 'ode'\n",
    "sampling.predictor = 'euler_maruyama'\n",
    "sampling.corrector = 'none'\n",
    "\n",
    "config.data = data = config_dict.ConfigDict()\n",
    "data.centered = False\n",
    "data.uniform_dequantization = False\n",
    "data.dataset = \"iris\" # shoppers\n",
    "data.image_size = 7  # 77\n",
    "\n",
    "config.model = model = config_dict.ConfigDict()\n",
    "model.nf = 64\n",
    "model.hidden_dims = (256, 512, 1024, 1024, 512, 256)\n",
    "model.conditional = True\n",
    "model.embedding_type = 'fourier'\n",
    "model.fourier_scale = 16\n",
    "model.layer_type = 'concatsquash'\n",
    "model.name = 'ncsnpp_tabular'\n",
    "model.scale_by_sigma = False\n",
    "model.ema_rate = 0.9999\n",
    "model.activation = 'elu'\n",
    "model.sigma_min = 0.01\n",
    "model.sigma_max = 10.\n",
    "model.num_scales = 50\n",
    "model.alpha0 = 0.3\n",
    "model.beta0 = 0.95\n",
    "\n",
    "config.optim = optim = config_dict.ConfigDict()\n",
    "optim.weight_decay = 0\n",
    "optim.optimizer = 'Adam'\n",
    "optim.lr = 1e-2 # 2e-3\n",
    "optim.beta1 = 0.9\n",
    "optim.eps = 1e-8\n",
    "optim.warmup = 1000 # 5000\n",
    "optim.grad_clip = 1.\n",
    "\n"
   ],
   "metadata": {
    "id": "6NUg-JJOHiZt",
    "ExecuteTime": {
     "end_time": "2024-06-02T09:13:47.251284Z",
     "start_time": "2024-06-02T09:13:47.244169Z"
    }
   },
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "source": [
    "#@title Build a score mode network and dataset\n",
    "\n",
    "score_model = mutils.create_model(config)\n",
    "num_params = sum(p.numel() for p in score_model.parameters())\n",
    "\n",
    "# Build data iterators\n",
    "train_ds, eval_ds, (transformer, meta) = datasets.get_dataset(config,\n",
    "                                            uniform_dequantization=config.data.uniform_dequantization)\n",
    "\n",
    "train_iter = DataLoader(train_ds, batch_size=config.training.batch_size)\n",
    "# eval_iter = iter(DataLoader(eval_ds, batch_size=config.eval.batch_size))  # pytype: disable=wrong-arg-types\n"
   ],
   "metadata": {
    "id": "-24lvCha8vz3",
    "ExecuteTime": {
     "end_time": "2024-06-02T09:13:49.693691Z",
     "start_time": "2024-06-02T09:13:49.652877Z"
    }
   },
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "source": [
    "#@title Setup SDEs\n",
    "\n",
    "# Setup SDEs\n",
    "if config.training.sde.lower() == 'vpsde':\n",
    "  sde = sde_lib.VPSDE(beta_min=config.model.beta_min, beta_max=config.model.beta_max, N=config.model.num_scales)\n",
    "  sampling_eps = 1e-3\n",
    "elif config.training.sde.lower() == 'subvpsde':\n",
    "  sde = sde_lib.subVPSDE(beta_min=config.model.beta_min, beta_max=config.model.beta_max, N=config.model.num_scales)\n",
    "  sampling_eps = 1e-3\n",
    "elif config.training.sde.lower() == 'vesde':\n",
    "  sde = sde_lib.VESDE(sigma_min=config.model.sigma_min, sigma_max=config.model.sigma_max, N=config.model.num_scales)\n",
    "  sampling_eps = 1e-5\n",
    "else:\n",
    "  raise NotImplementedError(f\"SDE {config.training.sde} unknown.\")\n"
   ],
   "metadata": {
    "id": "RZxx5R9qJggt",
    "ExecuteTime": {
     "end_time": "2024-06-02T09:13:51.460538Z",
     "start_time": "2024-06-02T09:13:51.454966Z"
    }
   },
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "source": [
    "#@title Build utilities for training\n",
    "tb_dir = os.path.join(config.workdir, \"tensorboard\")\n",
    "os.makedirs(tb_dir, exist_ok=True)\n",
    "writer = tensorboard.SummaryWriter(tb_dir)\n",
    "\n",
    "ema = ExponentialMovingAverage(score_model.parameters(), decay=config.model.ema_rate)\n",
    "optimizer = losses.get_optimizer(config, score_model.parameters()) # Adam optimizer, lr 2e-3\n",
    "state = dict(optimizer=optimizer, model=score_model, ema=ema, step=0, epoch=0)\n",
    "\n",
    "checkpoint_dir = os.path.join(config.workdir, \"checkpoints\")\n",
    "checkpoint_meta_dir = os.path.join(config.workdir, \"checkpoints-meta\", \"checkpoint.pth\")\n",
    "checkpoint_finetune_dir = os.path.join(config.workdir, \"checkpoints_finetune\")\n",
    "\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "os.makedirs(os.path.dirname(checkpoint_meta_dir), exist_ok=True)\n",
    "os.makedirs(checkpoint_finetune_dir, exist_ok=True)\n",
    "\n",
    "scaler = datasets.get_data_scaler(config)\n",
    "inverse_scaler = datasets.get_data_inverse_scaler(config)\n",
    "\n",
    "optimize_fn = losses.optimization_manager(config)\n",
    "continuous = config.training.continuous\n",
    "reduce_mean = config.training.reduce_mean\n",
    "likelihood_weighting = config.training.likelihood_weighting\n",
    "\n",
    "def loss_fn(model, batch):\n",
    "  score_fn = mutils.get_score_fn(sde, model, train=True, continuous=continuous)\n",
    "  t = torch.rand(batch.shape[0], device=batch.device) * (sde.T - 1e-5) + 1e-5\n",
    "  z = torch.randn_like(batch)\n",
    "  mean, std = sde.marginal_prob(batch, t)\n",
    "  perturbed_data = mean + std[:, None] * z\n",
    "\n",
    "  score = score_fn(perturbed_data, t)\n",
    "\n",
    "  loss_values = torch.square(score * std[:, None] + z)\n",
    "  loss_values = torch.mean(loss_values.reshape(loss_values.shape[0], -1), dim=-1)\n",
    "\n",
    "  return loss_values\n",
    "\n",
    "# Building sampling functions\n",
    "sampling_shape = (config.training.batch_size, config.data.image_size)\n",
    "sampling_fn = sampling_.get_sampling_fn(config, sde, sampling_shape, inverse_scaler, sampling_eps)\n",
    "\n",
    "scores_max = 0\n",
    "\n",
    "likelihood_fn = likelihood.get_likelihood_fn(sde, inverse_scaler)\n",
    "sampling_shape = (train_ds.shape[0], config.data.image_size)\n"
   ],
   "metadata": {
    "id": "4EgKaRhWInXp",
    "ExecuteTime": {
     "end_time": "2024-06-02T09:13:53.658755Z",
     "start_time": "2024-06-02T09:13:53.650465Z"
    }
   },
   "outputs": [],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "source": [
    "#@title Build utilities for v scheduling\n",
    "\n",
    "def min_max_scaling(factor, scale=(0, 1)):\n",
    "\n",
    "  std = (factor - factor.min()) / (factor.max() - factor.min())\n",
    "  new_min = torch.tensor(scale[0])\n",
    "  new_max = torch.tensor(scale[1])\n",
    "  return std * (new_max - new_min) + new_min\n",
    "\n",
    "\n",
    "def compute_v(ll, alpha, beta):\n",
    "\n",
    "  v = -torch.ones(ll.shape).to(ll.device)\n",
    "  v[torch.gt(ll, beta)] = torch.tensor(0., device=v.device)\n",
    "  v[torch.le(ll, alpha)] = torch.tensor(1., device=v.device)\n",
    "\n",
    "  if ll[torch.eq(v, -1)].shape[0] !=0 and ll[torch.eq(v, -1)].shape[0] !=1 :\n",
    "        v[torch.eq(v, -1)] = min_max_scaling(ll[torch.eq(v, -1)], scale=(1, 0)).to(v.device)\n",
    "  else:\n",
    "        v[torch.eq(v, -1)] = torch.tensor(0.5, device=v.device)\n",
    "\n",
    "  return v"
   ],
   "metadata": {
    "id": "IrnN2b_31mGj",
    "ExecuteTime": {
     "end_time": "2024-06-02T09:13:58.938404Z",
     "start_time": "2024-06-02T09:13:58.930699Z"
    }
   },
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "source": [
    "#@title Start model training\n",
    "alpha0 = config.model.alpha0\n",
    "beta0 = config.model.beta0\n",
    "\n",
    "for epoch in range(config.training.epoch+1):\n",
    "  state['epoch'] += 1\n",
    "  for iteration, batch in enumerate(train_iter):\n",
    "    print(len(train_iter), iteration)\n",
    "    batch = batch.to(config.device).float()\n",
    "    # loss = train_step_fn(state, batch)\n",
    "\n",
    "    # model = state['model']\n",
    "    optimizer = state['optimizer']\n",
    "    optimizer.zero_grad()\n",
    "    loss_values = loss_fn(score_model, batch)\n",
    "\n",
    "    q_alpha = torch.tensor(alpha0 + torch.log( torch.tensor(1+ 0.0001718*state['step']* (1-alpha0), dtype=torch.float32) )).clamp_(max=1).to(loss_values.device)\n",
    "    q_beta = torch.tensor(beta0 + torch.log( torch.tensor(1+ 0.0001718*state['step']* (1-beta0), dtype=torch.float32) )).clamp_(max=1).to(loss_values.device)\n",
    "\n",
    "    alpha = torch.quantile(loss_values, q_alpha)\n",
    "    beta = torch.quantile(loss_values, q_beta)\n",
    "    assert alpha <= beta\n",
    "    v = compute_v(loss_values, alpha, beta)\n",
    "    loss = torch.mean(v*loss_values)\n",
    "\n",
    "    loss.backward()\n",
    "    optimize_fn(optimizer, score_model.parameters(), step=state['step'])\n",
    "    state['step'] += 1\n",
    "    state['ema'].update(score_model.parameters())\n",
    "\n",
    "  print(\"epoch: %d, iter: %d, training_loss: %.5e, q_alpha: %.3e, q_beta: %.3e\" % (epoch, iteration, loss.item(), q_alpha, q_beta))\n",
    "  if epoch % 10 == 0:\n",
    "    save_checkpoint(checkpoint_meta_dir, state)\n"
   ],
   "metadata": {
    "id": "dYLEiPehLV0F",
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-06-02T09:14:01.126644Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51608/3669351031.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  q_alpha = torch.tensor(alpha0 + torch.log( torch.tensor(1+ 0.0001718*state['step']* (1-alpha0), dtype=torch.float32) )).clamp_(max=1).to(loss_values.device)\n",
      "/tmp/ipykernel_51608/3669351031.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  q_beta = torch.tensor(beta0 + torch.log( torch.tensor(1+ 0.0001718*state['step']* (1-beta0), dtype=torch.float32) )).clamp_(max=1).to(loss_values.device)\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "#@title Start fine-tune the pre-trained model\n",
    "\n",
    "hutchinson_type = config.training.hutchinson_type\n",
    "tolerance = config.training.tolerance\n",
    "\n",
    "likelihood_fn = likelihood.get_likelihood_fn(sde, inverse_scaler, hutchinson_type, tolerance, tolerance)\n",
    "\n",
    "train_ds_tensor = torch.tensor(train_ds, device=config.device, dtype=torch.float32)\n",
    "train_ll = likelihood_fn(score_model, train_ds_tensor, eps_iters = config.training.eps_iters)[0]\n",
    "\n",
    "if config.training.retrain_type == 'median':\n",
    "  idx = torch.where(train_ll <= torch.median(train_ll), True, False)\n",
    "elif config.training.retrain_type == 'mean':\n",
    "  idx = torch.where(train_ll <= torch.mean(train_ll), True, False)\n",
    "\n",
    "re_train = train_ds_tensor[idx]\n",
    "train_iter = DataLoader(re_train, batch_size=config.training.batch_size)\n",
    "step = 0\n",
    "\n",
    "# model = state['model']\n",
    "samples, n = sampling_fn(score_model, sampling_shape=sampling_shape)\n",
    "samples = apply_activate(samples, transformer.output_info)\n",
    "samples = transformer.inverse_transform(samples.cpu().numpy())\n",
    "scores_max = 0\n",
    "\n",
    "for epoch in range(config.training.fine_tune_epochs):\n",
    "  for iteration, batch in enumerate(train_iter):\n",
    "    batch = batch.to(config.device).float()\n",
    "\n",
    "    optimizer = state['optimizer']\n",
    "    optimizer.zero_grad()\n",
    "    loss_values = loss_fn(score_model, batch)\n",
    "    loss = torch.mean(loss_values)\n",
    "\n",
    "    state['step'] += 1\n",
    "\n",
    "    loss.backward()\n",
    "    optimize_fn(optimizer, score_model.parameters(), step=state['step'])\n",
    "    state['step'] += 1\n",
    "    state['ema'].update(score_model.parameters())\n",
    "  train_ll_after = likelihood_fn(score_model, train_ds_tensor, eps_iters = config.training.eps_iters)[0]\n",
    "\n",
    "  diff = train_ll_after - train_ll\n",
    "  idx_after = torch.where(diff < -0.1, True, False)\n",
    "  re_train = train_ds_tensor[idx_after]\n",
    "\n",
    "  train_iter = DataLoader(re_train, batch_size=config.training.batch_size)\n",
    "\n",
    "  save_checkpoint(os.path.join(checkpoint_finetune_dir, \"checkpoint_finetune.pth\"), state)\n",
    "  print(\"epoch: %d, iter: %d, finetuning_loss: %.5e\" % (epoch, iteration, loss.item()))\n",
    "\n"
   ],
   "metadata": {
    "id": "zksXlEWI3WiZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#@title Start evaluating the model after the training\n",
    "\n",
    "samples, n = sampling_fn(score_model, sampling_shape=sampling_shape)\n",
    "samples = apply_activate(samples, transformer.output_info)\n",
    "samples = transformer.inverse_transform(samples.cpu().numpy())\n",
    "\n",
    "eval_samples = transformer.inverse_transform(eval_ds)\n",
    "train_samples = transformer.inverse_transform(train_ds)\n",
    "\n",
    "scores, _ = evaluation.compute_scores(train=train_samples, test=eval_samples, synthesized_data=[samples], metadata=meta)\n"
   ],
   "metadata": {
    "id": "YkxVQ36R4PAw"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(scores)"
   ],
   "metadata": {
    "id": "kIWGI9thDCnm",
    "outputId": "fb5192c1-9361-4aa9-9864-d323f2f8070e",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "binary_f1      0.220284\n",
      "roc_auc        0.488710\n",
      "weighted_f1    0.265565\n",
      "accuracy       0.420114\n",
      "dtype: float64\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "KO3vcZw5EjDd"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
